{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big-data analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying big-data analytics, data science, and machine learning (ML) applications in real-world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in real-world (estimation), \n",
    "* Analytics-tuning and model-training is only around 25% of the work. \n",
    "* Approximately 50% of the effort goes into making data ready for analytics and ML. \n",
    "* The remaining 25% effort goes into making insights and model inferences easily consumable at scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the data pipeline and an overview of big data architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perspective: \n",
    "    \n",
    "  By understanding the perspectives of all stakeholders, you can enhance the impact of your work.\n",
    "* Pipeline: \n",
    "    \n",
    "  In this section, you will learn about the conceptual stages of a big data pipeline passing through the data lake and the data   warehouse.\n",
    "* Possibilities: \n",
    "\n",
    "  In this section,get a glimpse of serverless pipelines on AWS, Azure, and Google Cloud.\n",
    "* Production: \n",
    "\n",
    "  This section offers tips for your big data pipeline deployment to be successful in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three stakeholders involved in building data analytics or machine learning applications: \n",
    "* Data scientists, \n",
    "\n",
    "He aim is to find the most robust and computationally least expensive model for a given problem using available data.\n",
    "* Engineers, \n",
    "\n",
    "The aim is to build things that others can depend on; to innovate either by building new things or finding better ways to build existing things that function 24x7 without much human intervention.\n",
    "\n",
    "* Business managers.\n",
    "\n",
    "he aim is to deliver value to customers; science and engineering are means to that end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired engineering characteristics of a data pipeline are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accessibility: \n",
    "    \n",
    "  Data being easily accessible to data scientists for hypothesis evaluation and model experimentation, preferably through a     query language.\n",
    "\n",
    "* Scalability: \n",
    "    \n",
    "  The ability to scale as the amount of ingested data increases, while keeping the cost low.\n",
    "\n",
    "* Efficiency: \n",
    "\n",
    "  data and machine learning results being ready within the specified latency to meet the business objectives.\n",
    "\n",
    "* Monitoring:\n",
    "    \n",
    "  automatic alerts about the health of the data and the pipeline, needed for proactive response to potential business risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A data pipeline \n",
    "Stitches together the end-to-end operation consisting of collecting the data, transforming it into insights, training a model, delivering insights, applying the model whenever and wherever the action needs to be taken to achieve the business goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pipeline has five stages \n",
    "\n",
    "#### Collection: \n",
    "\n",
    "Data sources (mobile apps, websites, web apps, microservices, IoT devices etc.) are instrumented to collect relevant data.\n",
    "\n",
    "#### Ingestion: \n",
    "\n",
    "The instrumented sources pump the data into various inlet points (HTTP, MQTT, message queue etc.). There can also be jobs to import data from services like Google Analytics. The data can be in two forms: blobs and streams. All this data gets collected into a Data Lake.\n",
    "\n",
    "#### Preparation: \n",
    "\n",
    "It is the extract, transform, load (ETL) operation to cleanse, conform, shape, transform, and catalog the data blobs and streams in the data lake; making the data ready-to-consume for ML and store it in a Data Warehouse.\n",
    "\n",
    "#### Computation:\n",
    "\n",
    "This is where analytics, data science and machine learning happen. Computation can be a combination of batch and stream processing. Models and insights (both structured data and streams) are stored back in the Data Warehouse.\n",
    "\n",
    "#### Presentation: \n",
    "\n",
    "The insights are delivered through dashboards, emails, SMSs, push notifications, and microservices. The ML model inferences are exposed as microservices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture is a trade-off between performance and cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda architecture \n",
    "* Offering different performance and cost tradeoffs\n",
    "\n",
    "* The underlying assumption in the lambda architecture is that the source data model is append-only, i.e. ingested events are timestamped and appended to existing events, and never overwritten.\n",
    "\n",
    "####  Lambda architecture consists of three layers:\n",
    "\n",
    "* Batch Layer: \n",
    "    \n",
    "  offers high throughput, comprehensive, economical map-reduce batch processing, but higher latency.\n",
    "* Speed Layer: \n",
    "\n",
    "  offers low latency real-time stream processing, but costlier and may overshoot memory limit when data volume is high.\n",
    "* Serving Layer: \n",
    "\n",
    "  The output from high throughput batch processing, when ready, is merged with the output of the stream processing to provide comprehensive results in the form of pre-computed views or ad-hoc queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Architecture: Your choice of the stack on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An architecture of the data pipeline using open source technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['OPENSOURCE'](opensource.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP / MQTT \n",
    "Endpoints for ingesting data, and also for serving the results. There are several frameworks and technologies for this.\n",
    "#### Pub/Sub Message Queue \n",
    "for ingesting high-volume streaming data. Kafka is currently the de-facto choice. It is battle-proven to scale to high event ingestion rate.\n",
    "#### Low-Cost High-Volume Data Store for data lake (and data warehouse), \n",
    "Hadoop HDFS or cloud blob storage like AWS S3.\n",
    "#### Query and Catalog Infrastructure for converting a data lake into a data warehouse,\n",
    "Apache Hive  is a popular query language choice. or Pyspark  \n",
    "#### Map-Reduce Batch Compute engine for high throughput processing, \n",
    "e.g. Hadoop Map-Reduce, Apache Spark.\n",
    "#### Stream Compute for latency-sensitive processing, \n",
    "e.g. Apache Storm, Apache Flink. Apache Beam is an emerging choice for writing compute data-flow, and can be deployed on a Spark batch runner or Flink stream runner.\n",
    "#### Machine Learning Frameworks for data science and ML.\n",
    "Scikit-Learn, TensorFlow,MLLIB, and PyTorch are a popular choice for implementing machine learning.\n",
    "#### Low-Latency Data Stores for storing the results. \n",
    "There are too many well-established choices of data stores depending on data type, performance, scale and cost to cover here.\n",
    "#### Deployment orchestration\n",
    "options are Hadoop YARN, Kubernetes / Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Architecture: Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical serverless architectures of big data pipelines on \n",
    "* Amazon Web Services, \n",
    "* Microsoft Azure, \n",
    "* Google Cloud Platform (GCP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless big data pipeline architecture on Amazon Web Services (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['aws'](aws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless big data pipeline architecture on Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['azure'](azure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless big data pipeline architecture on Google Cloud Platform (GCP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['gcp'](gcp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# large-scale data processing Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100GB Dataset\n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask DataFrame â€” Flexible parallel computing library for analytics. \n",
    "\n",
    "https://docs.dask.org/en/latest/dataframe.html\n",
    "\n",
    "* PySpark â€” A unified analytics engine for large-scale data processing based on Spark.\n",
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "* Koalas â€” Pandas API on Apache Spark.\n",
    "https://koalas.readthedocs.io/en/latest/index.html\n",
    "\n",
    "* Vaex â€” A Python library for lazy Out-of-Core dataframes.\n",
    "https://vaex.readthedocs.io/en/latest/\n",
    "\n",
    "* Turicreate â€” A relatively clandestine machine learning package with its dataframe structure â€” SFrame, which qualifies.\n",
    "https://github.com/apple/turicreate\n",
    "\n",
    "* Datatable â€” The backbone of H2Oâ€™s Driverless.ai. A dataframe package with specific emphasis on speed and big data support for a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* H2O â€” The standard in-memory dataframe is well-rounded. Still, with the recommendations of a cluster four times the size of the dataset, you need deep pockets to use it for exploration and development.\n",
    "\n",
    "* cuDF (RapidAI) â€” A GPU dataframe package is an exciting concept. For big data, you must use distributed GPUs with Dask to match your data size, perfect for bottomless pockets.\n",
    "\n",
    "* Modin â€” A tool to scale Pandas without changes to the API which uses Dask or Ray in the backend. Sadly at this moment, it can only read a single parquet file while I already had a chunked parquet dataset. With the prospect of getting similar results as * \n",
    "\n",
    "* Dask DataFrame, it didnâ€™t seem to be worth pursuing by merging all parquet files to a single one at this point.\n",
    "\n",
    "* Vaex does have a GPU and numba support for heavy calculations which I did not benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beyond-pandas-spark-dask-vaex-and-other-big-data-technologies-battling-head-to-head-a453a1f8cc13"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Credits \n",
    "Satish Chandra Gupta-Cofounder @ Slang Labs | Ex Amazon, Microsoft Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Big data  architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uberâ€™s Machine Learning Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['uber'](uber.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://eng.uber.com/michelangelo-machine-learning-platform/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotify  generate the â€˜Discover Weeklyâ€™ personalized music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['spotify'](spotify1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['netflix'](netflix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other arc\n",
    "\n",
    "https://keen.io/blog/architecture-of-giants-data-stacks-at-facebook-netflix-airbnb-and-pinterest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
