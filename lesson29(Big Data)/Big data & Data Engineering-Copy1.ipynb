{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big-data   & Data Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Extremely large data sets that may be analysed computationally to reveal patterns, trends, and associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['bigdata'](big_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Build pipelines that prepare and transform data\n",
    "#### Data engineers set up and maintain the data infrastructures\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['workflow'](dataengworkflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the data pipeline and an overview of big data architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three stakeholders involved in building data analytics or machine learning applications: \n",
    "* Data scientists, \n",
    "\n",
    "He aim is to find the most robust and computationally least expensive model for a given problem using available data.\n",
    "* Engineers, \n",
    "\n",
    "The aim is to build things that others can depend on; to innovate either by building new things or finding better ways to build existing things that function 24x7 without much human intervention.\n",
    "\n",
    "* Business managers.\n",
    "\n",
    "he aim is to deliver value to customers; science and engineering are means to that end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired engineering characteristics of a data pipeline are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accessibility: \n",
    "    \n",
    "  Data being easily accessible to data scientists for hypothesis evaluation and model experimentation, preferably through a     query language.\n",
    "\n",
    "* Scalability: \n",
    "    \n",
    "  The ability to scale as the amount of ingested data increases, while keeping the cost low.\n",
    "\n",
    "* Efficiency: \n",
    "\n",
    "  data and machine learning results being ready within the specified latency to meet the business objectives.\n",
    "\n",
    "* Monitoring:\n",
    "    \n",
    "  automatic alerts about the health of the data and the pipeline, needed for proactive response to potential business risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A data pipeline \n",
    "Stitches together the end-to-end operation consisting of collecting the data, transforming it into insights, training a model, delivering insights, applying the model whenever and wherever the action needs to be taken to achieve the business goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pipeline has five stages \n",
    "\n",
    "#### Collection: \n",
    "\n",
    "Data sources (mobile apps, websites, web apps, microservices, IoT devices etc.) are instrumented to collect relevant data.\n",
    "\n",
    "#### Ingestion: \n",
    "\n",
    "The instrumented sources pump the data into various inlet points (HTTP, MQTT, message queue etc.). There can also be jobs to import data from services like Google Analytics. The data can be in two forms: blobs and streams. All this data gets collected into a Data Lake.\n",
    "\n",
    "#### Preparation: \n",
    "\n",
    "It is the extract, transform, load (ETL) operation to cleanse, conform, shape, transform, and catalog the data blobs and streams in the data lake; making the data ready-to-consume for ML and store it in a Data Warehouse.\n",
    "\n",
    "#### Computation:\n",
    "\n",
    "This is where analytics, data science and machine learning happen. Computation can be a combination of batch and stream processing. Models and insights (both structured data and streams) are stored back in the Data Warehouse.\n",
    "\n",
    "#### Presentation: \n",
    "\n",
    "The insights are delivered through dashboards, emails, SMSs, push notifications, and microservices. The ML model inferences are exposed as microservices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Architecture: Your choice of the stack on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An architecture of the data pipeline using open source technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['OPENSOURCE'](opensource.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP / MQTT \n",
    "Endpoints for ingesting data, and also for serving the results. There are several frameworks and technologies for this.\n",
    "#### Pub/Sub Message Queue \n",
    "for ingesting high-volume streaming data. Kafka is currently the de-facto choice. It is battle-proven to scale to high event ingestion rate.\n",
    "#### Low-Cost High-Volume Data Store for data lake (and data warehouse), \n",
    "Hadoop HDFS or cloud blob storage like AWS S3.\n",
    "#### Query and Catalog Infrastructure for converting a data lake into a data warehouse,\n",
    "Apache Hive  is a popular query language choice. or Pyspark  \n",
    "#### Map-Reduce Batch Compute engine for high throughput processing, \n",
    "e.g. Hadoop Map-Reduce, Apache Spark.\n",
    "#### Stream Compute for latency-sensitive processing, \n",
    "e.g. Apache Storm, Apache Flink. Apache Beam is an emerging choice for writing compute data-flow, and can be deployed on a Spark batch runner or Flink stream runner.\n",
    "#### Machine Learning Frameworks for data science and ML.\n",
    "Scikit-Learn, TensorFlow,MLLIB, and PyTorch are a popular choice for implementing machine learning.\n",
    "#### Low-Latency Data Stores for storing the results. \n",
    "There are too many well-established choices of data stores depending on data type, performance, scale and cost to cover here.\n",
    "#### Deployment orchestration\n",
    "options are Hadoop YARN, Kubernetes / Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Architecture: Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical serverless architectures of big data pipelines on \n",
    "* Amazon Web Services, \n",
    "* Microsoft Azure, \n",
    "* Google Cloud Platform (GCP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless big data pipeline architecture on Amazon Web Services (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['aws'](aws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless big data pipeline architecture on Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['azure'](azure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless big data pipeline architecture on Google Cloud Platform (GCP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['gcp'](gcp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# large-scale data processing Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100GB Dataset\n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask DataFrame — Flexible parallel computing library for analytics. \n",
    "\n",
    "https://docs.dask.org/en/latest/dataframe.html\n",
    "\n",
    "* PySpark — A unified analytics engine for large-scale data processing based on Spark.\n",
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "* Koalas — Pandas API on Apache Spark.\n",
    "https://koalas.readthedocs.io/en/latest/index.html\n",
    "\n",
    "* Vaex — A Python library for lazy Out-of-Core dataframes.\n",
    "https://vaex.readthedocs.io/en/latest/\n",
    "\n",
    "* Turicreate — A relatively clandestine machine learning package with its dataframe structure — SFrame, which qualifies.\n",
    "https://github.com/apple/turicreate\n",
    "\n",
    "* Datatable — The backbone of H2O’s Driverless.ai. A dataframe package with specific emphasis on speed and big data support for a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* H2O — The standard in-memory dataframe is well-rounded. Still, with the recommendations of a cluster four times the size of the dataset, you need deep pockets to use it for exploration and development.\n",
    "\n",
    "* cuDF (RapidAI) — A GPU dataframe package is an exciting concept. For big data, you must use distributed GPUs with Dask to match your data size, perfect for bottomless pockets.\n",
    "\n",
    "* Modin — A tool to scale Pandas without changes to the API which uses Dask or Ray in the backend. Sadly at this moment, it can only read a single parquet file while I already had a chunked parquet dataset. With the prospect of getting similar results as * \n",
    "\n",
    "* Dask DataFrame, it didn’t seem to be worth pursuing by merging all parquet files to a single one at this point.\n",
    "\n",
    "* Vaex does have a GPU and numba support for heavy calculations which I did not benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beyond-pandas-spark-dask-vaex-and-other-big-data-technologies-battling-head-to-head-a453a1f8cc13"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Credits \n",
    "Satish Chandra Gupta-Cofounder @ Slang Labs | Ex Amazon, Microsoft Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Big data  architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber’s Machine Learning Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['uber'](uber.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://eng.uber.com/michelangelo-machine-learning-platform/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotify  generate the ‘Discover Weekly’ personalized music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['spotify'](spotify1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['netflix'](netflix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other arc\n",
    "\n",
    "https://keen.io/blog/architecture-of-giants-data-stacks-at-facebook-netflix-airbnb-and-pinterest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL is a process that extracts the data from different source systems, then transforms the data and finally loads the data into the Data Warehouse system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['netflix'](etlpic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from SQL  database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "\n",
    "uri = \"postgresql://repl:password@africadataschool:6000/john\"  \n",
    "\n",
    "db_engine = sqlalchemy.create_engine(uri)  \n",
    " \n",
    "pd.read_sql(\"SELECT * FROM student\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data into pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# Test the spark \n",
    "\n",
    "df = spark.read.format(\"csv\").load(\"dbfs:/FileStore/shared_uploads/africadataschool@outlook.com/student _data.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform operations \n",
    "\n",
    "* Filter\n",
    "* Selection & renaming\n",
    "* Gropping and Aggregation\n",
    "* joining \n",
    "* ordering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark DataFrame with student \n",
    "student_df \n",
    "# PySpark DataFrame with marks \n",
    "marks_df \n",
    "# Groupby marks\n",
    "marks_per_student = marks_df.groupBy(\"student_id\").mean(\"marks\") \n",
    " \n",
    "# Join on customer ID \n",
    "student_df.join( marks_per_student,  student_df.student_id==marks_per_student.student_id ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas .to_parquet() method \n",
    "df.to_parquet(\"./s3://path/to/bucket/student.parquet\") \n",
    "\n",
    "# PySpark .write.parquet() method \n",
    "\n",
    "df.write.parquet(\"./s3://path/to/bucket/customer.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into PostgreSQL database \n",
    "\n",
    "marks_df.to_sql(\"marks\",\n",
    "                db_engine, \n",
    "                schema=\"store\",\n",
    "                if_exists=\"replace\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a data Pipeline now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT\n",
    "def extract_StudentTable_to_df(tablename, db_engine):   \n",
    "    return pd.read_sql(\"SELECT * FROM {}\".format(tablename), db_engine) \n",
    "#TRAN \n",
    "def join_table_transform(df, column, pat, suffixes): \n",
    "    #   join table ...  \n",
    "    return transformeddf \n",
    "#LOAD\n",
    "def load_df_into_dwh(film_df, tablename, schema, db_engine):  \n",
    "    # load to datawarehouse \n",
    "    return pd.to_sql(tablename, db_engine, schema=schema, if_exists=\"replace\") \n",
    " \n",
    "db_engines = { ... } # Needs to be configured \n",
    "\n",
    "\n",
    "def etl_marks():   \n",
    "  # Extract   \n",
    "   film_df = extract_table_to_df(\"film\", db_engines[\"store\"])  \n",
    "\n",
    "\n",
    "   # Transform   \n",
    "   film_df = split_columns_transform(film_df, \"rental_rate\", \".\", [\"_dollar\", \"_cents\"])  \n",
    "\n",
    "  # Load   \n",
    "   load_df_into_dwh(film_df, \"film\", \"store\", db_engines[\"dwh\"]) \n",
    "    \n",
    "    return marks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIRFLOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow is a platform created by the community to programmatically author, schedule and monitor workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['netflix'](airdag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG \n",
    "from airflow.operators.python_operator import PythonOperator \n",
    "\n",
    "dag = DAG(dag_id=\"etl_pipeline\",\n",
    "          schedule_interval=\"0 0 * * *\")  \n",
    "\n",
    "etl_task = PythonOperator(task_id=\"etl_task\",\n",
    "                          python_callable=etl_marks, \n",
    "                          dag=dag)  \n",
    "\n",
    "send_email_heamaster=PythonOperator(task_id=\"etl_task\",\n",
    "                          python_callable=etl_marks, \n",
    "                          dag=dag)  \n",
    "\n",
    "\n",
    "\n",
    "etl_task.set_downstream(wait_for_this_task) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
